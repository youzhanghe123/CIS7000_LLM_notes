1. DPO与PPO的核心区别
DPO (Direct Preference Optimization)

目标：基于人类偏好数据，直接优化模型的行为，使其更符合偏好。
应用场景：通常用于 语言模型优化，如通过人类反馈提升生成文本的质量（比如 ChatGPT 的 RLHF 阶段）。
特点：DPO 不依赖明确的强化学习奖励函数，而是直接通过比较生成的样本与人类偏好样本之间的相对优劣来更新模型。

PPO (Proximal Policy Optimization)

目标：在不让策略发生剧烈变化的情况下，最大化预定义的奖励函数。
应用场景：更常见于经典的强化学习任务，比如游戏控制、机器人操作等。
特点：PPO 引入了「裁剪」机制，限制每次策略更新的步长，防止策略变化太大而导致性能下降。

2. 优化方式
DPO

假设有一组模型输出：一个被偏好（例如：人类标注为「更好」），另一个不被偏好，
优化目标是提升被偏好样本的生成概率，降低不被偏好样本的生成概率。
使用的损失函数类似于对比学习 （参见loss_function_pair_wise.png).

PPO

基于裁剪的策略优化：
直接更新策略 
𝜋
𝜃
π 
θ
​
 ，目标是最大化策略与旧策略相比的收益。
PPO 的核心损失函数包含一个裁剪项，用于限制策略变化：
𝐿
PPO
=
min
⁡
(
𝑟
𝑡
(
𝜃
)
𝐴
𝑡
,
clip
(
𝑟
𝑡
(
𝜃
)
,
1
−
𝜖
,
1
+
𝜖
)
𝐴
𝑡
)
L 
PPO
​
 =min(r 
t
​
 (θ)A 
t
​
 ,clip(r 
t
​
 (θ),1−ϵ,1+ϵ)A 
t
​
 )
其中：
𝑟
𝑡
(
𝜃
)
=
𝜋
𝜃
(
𝑎
𝑡
∣
𝑠
𝑡
)
𝜋
old
(
𝑎
𝑡
∣
𝑠
𝑡
)
r 
t
​
 (θ)= 
π 
old
​
 (a 
t
​
 ∣s 
t
​
 )
π 
θ
​
 (a 
t
​
 ∣s 
t
​
 )
​
  表示新旧策略的概率比；
𝐴
𝑡
A 
t
​
  是优势函数，衡量动作的相对好坏；
𝜖
ϵ 是裁剪范围。


