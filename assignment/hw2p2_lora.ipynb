{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfa8rujtIfWR"
      },
      "source": [
        "<h1 align=\"center\" style=\"color:green;font-size: 3em;\">Homework 2:\n",
        "Implementing Fine-tuning Techniques</h1>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u2emUkuIwLm"
      },
      "source": [
        "# Part 1: Introduction\n",
        "\n",
        "In this homework, you will implement various fine-tuning methods as described in different papers, specifically LoRA and IA3, and answer some conceptual questions about these techniques. Additionally, you will get an introduction to Hugging Face, a platform offering a wide range of models and datasets.\n",
        "\n",
        "**Instructions:**\n",
        "- Follow the notebook sections to implement various fine-tuning techniques.\n",
        "- Complete the code cells marked with `TODO`.\n",
        "- Ensure your code runs correctly by the end of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywkMu4GsJSNu"
      },
      "source": [
        "# Part 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cVHARZP6Nx_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b302c4-5c9f-4f61-b858-da9318d0afb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/472.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqY-ceD7BAff",
        "outputId": "f2a2d632-354c-470d-b400-e34829fb3158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.0+cu121\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# importing required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from typing import List\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, T5Tokenizer, T5ForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "th0qaOHclWwv"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rDVYvMkAHck"
      },
      "source": [
        "# Part 5: LoRA Adapters\n",
        "\n",
        "In this section, we will implement LoRA (Low-Rank Adaptation) and inject it into our causal model. Specifically, we will inject LoRA into the **key, query, and value** matrices of each transformer block.\n",
        "\n",
        "Recall from the LoRA paper that LoRA enhances model training efficiency by reducing the need to retrain all pretrained weights. Instead, it introduces two smaller matrices, A and B, which capture the necessary adaptations for the new task. This significantly reduces computational overhead while maintaining high performance.\n",
        "\n",
        "For more information, read the [paper](https://arxiv.org/pdf/2106.09685).\n",
        "\n",
        "By using LoRA in our causal model, we aim to achieve efficient fine-tuning with minimal computational cost, focusing on the key, query, and value matrices within each transformer block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIZMnzx_JbbB"
      },
      "source": [
        "## 5.1 LoRA class\n",
        "\n",
        "<h3>Task:</h3>\n",
        "First, let's implement the LoRA class based on how it is defined in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "WrorfDnBPLtd"
      },
      "outputs": [],
      "source": [
        "class LoRALayer():\n",
        "    def __init__(\n",
        "        self,\n",
        "        r: int,\n",
        "        lora_alpha: int,\n",
        "        lora_dropout: float,\n",
        "    ):\n",
        "        self.r = r\n",
        "        self.lora_alpha = lora_alpha\n",
        "        # Optional dropout\n",
        "        if lora_dropout > 0.:\n",
        "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
        "        else:\n",
        "            self.lora_dropout = lambda x: x\n",
        "\n",
        "class LoRAAdapter(nn.Module, LoRALayer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        existing_layer: nn.Module,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        r: int = 0,\n",
        "        lora_alpha: int = 1,\n",
        "        lora_dropout: float = 0.,\n",
        "        **kwargs\n",
        "    ):\n",
        "        nn.Module.__init__(self)\n",
        "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n",
        "        self.existing_layer = existing_layer\n",
        "\n",
        "        ## TODO: Finish this\n",
        "        # Check the dtype of the existing layer\n",
        "        existing_dtype = None\n",
        "        for param in existing_layer.parameters():\n",
        "            existing_dtype = param.dtype\n",
        "            break\n",
        "\n",
        "        self.r = r\n",
        "        if r > 0:\n",
        "            self.lora_A = nn.Parameter(torch.zeros(r, in_features,dtype=existing_dtype))\n",
        "            self.lora_B = nn.Parameter(torch.zeros(out_features, r, dtype=existing_dtype))\n",
        "            self.scaling = self.lora_alpha / self.r\n",
        "        self.reset_parameters()\n",
        "\n",
        "    ## TODO: Resets the two matrices (A and B) based on how the paper does it\n",
        "    def reset_parameters(self):\n",
        "        if self.r > 0:\n",
        "            # Initialize A with scaled-down kaiming uniform\n",
        "            torch.nn.init.normal_(self.lora_A, mean=0, std=0.02)\n",
        "            # Initialize B as zeros\n",
        "            torch.nn.init.zeros_(self.lora_B)\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        self.existing_layer.train(mode)\n",
        "\n",
        "    ## TODO: Finish this method\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        if self.r > 0:\n",
        "            # Original layer transformation\n",
        "            original_output = self.existing_layer(x)\n",
        "\n",
        "            # LoRA transformation\n",
        "            # 1. Apply dropout\n",
        "            lora_input = self.lora_dropout(x)\n",
        "            # 2. First low-rank transformation (Ax)\n",
        "            lora_output = F.linear(lora_input, self.lora_A)\n",
        "            # 3. Second low-rank transformation (BAx)\n",
        "            lora_output = F.linear(lora_output, self.lora_B)\n",
        "            # 4. Scale the output\n",
        "            return original_output + (lora_output * self.scaling)\n",
        "        # If r = 0, just use the original layer\n",
        "        return self.existing_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjlA3WsZJdaY"
      },
      "source": [
        "## 5.2 Inject into the model\n",
        "\n",
        "Recall in LoRA that we want to freeze the pre-trained model and only train our adapter weights `lora_A` and `lora_B`.  \n",
        "\n",
        "<hr>\n",
        "<h3>Task:</h3>\n",
        "\n",
        "Complete `mask_only_lora_as_trainable` so that only those weights require gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PbbbjJONPPHK"
      },
      "outputs": [],
      "source": [
        "# TODO: Finish the method\n",
        "def mark_only_lora_as_trainable(model: nn.Module) -> None:\n",
        "\n",
        "    # First, freeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Then, unfreeze only LoRA parameters\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LoRALayer):\n",
        "            if hasattr(module, 'lora_A'):\n",
        "                module.lora_A.requires_grad = True\n",
        "                print(\"lora_A\")\n",
        "            if hasattr(module, 'lora_B'):\n",
        "                module.lora_B.requires_grad = True\n",
        "                print(\"lora_B\")\n",
        "            # If using LoRA dropout, make sure its parameters are trainable\n",
        "            if hasattr(module, 'lora_dropout') and isinstance(module.lora_dropout, nn.Module):\n",
        "                for param in module.lora_dropout.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2pngOrH4omo"
      },
      "source": [
        "Finally, we want to write the code that will inject the LoRA adapters into our causal model.\n",
        "\n",
        "<hr>\n",
        "<h3>Task: </h3>\n",
        "\n",
        "Complete the following methods so that we can correctly inject our LoRA adapters into the model.\n",
        "\n",
        "`match_submodules`: Returns a list of names of layers in a model whose names match a specified key.\n",
        "\n",
        "`get_submodule`: Retrieves a specific submodule from a model based on its name.\n",
        "\n",
        "`replace_submodule`: Replaces a specific submodule in a model with a new module at a given path.\n",
        "\n",
        "```\n",
        "Code Hint:\n",
        "You can use the set_attr and get_attr methods to get and replace submodules.\n",
        "```\n",
        "\n",
        "\n",
        "`inject_adapter`: Replaces all submodules in a model that match any string in a list with a new module created by an adapter function.\n",
        "\n",
        "```\n",
        "Code Hint:\n",
        "Remember to put the adapters onto GPU\n",
        "```\n",
        "\n",
        "```\n",
        "Code Hint:\n",
        "Here is an example of `inject_adapter` usage:\n",
        "inject_adapter(model, [\"query_key_value\"], lambda x: LoRAAdapter(x, r=8,lora_alpha=8, in_features=x.in_features, out_features=x.out_features))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nn7xh3NrPRMO"
      },
      "outputs": [],
      "source": [
        "# TODO: Finish the method\n",
        "def match_submodules(model: nn.Module, key:str) -> List[str]:\n",
        "\n",
        "    result = []\n",
        "    for name, _ in model.named_modules():\n",
        "        if key in name:\n",
        "            result.append(name)\n",
        "    print(\"result \", result)\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_submodule(model: nn.Module, module_name:str):\n",
        "    return model.get_submodule(module_name)\n",
        "\n",
        "# TODO: Finish the method\n",
        "def replace_submodule(model: nn.Module, module_path: str, new_module):\n",
        "\n",
        "    path_parts = module_path.split('.')\n",
        "\n",
        "    if len(path_parts) == 1:\n",
        "        setattr(model, module_path, new_module)\n",
        "        return\n",
        "\n",
        "    # Get the parent module\n",
        "    parent_path = '.'.join(path_parts[:-1])\n",
        "    parent_module = get_submodule(model, parent_path)\n",
        "\n",
        "    # Replace the module in its parent\n",
        "    setattr(parent_module, path_parts[-1], new_module)\n",
        "\n",
        "\n",
        "# TODO: Finish the method\n",
        "def inject_adapter(model: nn.Module, match_on: List[str], adapter_fn):\n",
        "\n",
        "    # Find all modules that match any of the strings in match_on\n",
        "    matched_modules = []\n",
        "    print(match_on)\n",
        "    for key in match_on:\n",
        "        matched_modules.extend(match_submodules(model, key))\n",
        "    print(matched_modules)\n",
        "    # Replace each matched module with a LoRA adapter\n",
        "    for module_path in matched_modules:\n",
        "        # Get the original module\n",
        "        original_module = get_submodule(model, module_path)\n",
        "\n",
        "        # Create a new adapter module\n",
        "        new_module = adapter_fn(original_module)\n",
        "\n",
        "        # Move to the same device as the original module\n",
        "        if next(original_module.parameters(), None) is not None:\n",
        "            device = next(original_module.parameters()).device\n",
        "            new_module = new_module.to(device)\n",
        "\n",
        "        # Replace the original module with the adapter\n",
        "        replace_submodule(model, module_path, new_module)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBr6EXswJm4G"
      },
      "source": [
        "## 5.3 Evaluation on a benchmark\n",
        "\n",
        "Next, we want to inject the LoRA adapter into our causal model we defined earlier. Let's also check to see how many parameters are in this model, as well as how many of these parameters are considered trainable.\n",
        "\n",
        "<hr>\n",
        "<h3>Task:</h3>\n",
        "\n",
        "Re-initialize the causal model and chck the model architecture.\n",
        "\n",
        "```\n",
        "Code Hint:\n",
        "The name of the model is \"facebook/opt-125m\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HmKOW16I5x2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ff5e02-0762-4535-e89f-c992787425e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
              "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# TODO: Re-initialize the causal model\n",
        "causal_model_name =  \"facebook/opt-125m\"\n",
        "causal_model = AutoModelForCausalLM.from_pretrained(causal_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "causal_tokenizer = AutoTokenizer.from_pretrained(causal_model_name)\n",
        "\n",
        "# TODO: Check the model architecture\n",
        "causal_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrgoaUhyboZU"
      },
      "source": [
        "Next, we want to call the inject_adapter method on our causal model and see how this changed our model architecture.\n",
        "\n",
        "<hr>\n",
        "<h3>Task:</h3>\n",
        "\n",
        "Calculate and print the total number of parameters as well as the number of trainable parameters after we inject LoRA into our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "PDGcuV73B7M5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456cd85a-8c80-465f-bd3f-f6c376cc5999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['q_proj', 'k_proj', 'v_proj']\n",
            "result  ['model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.6.self_attn.q_proj', 'model.decoder.layers.7.self_attn.q_proj', 'model.decoder.layers.8.self_attn.q_proj', 'model.decoder.layers.9.self_attn.q_proj', 'model.decoder.layers.10.self_attn.q_proj', 'model.decoder.layers.11.self_attn.q_proj']\n",
            "result  ['model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.6.self_attn.k_proj', 'model.decoder.layers.7.self_attn.k_proj', 'model.decoder.layers.8.self_attn.k_proj', 'model.decoder.layers.9.self_attn.k_proj', 'model.decoder.layers.10.self_attn.k_proj', 'model.decoder.layers.11.self_attn.k_proj']\n",
            "result  ['model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.6.self_attn.v_proj', 'model.decoder.layers.7.self_attn.v_proj', 'model.decoder.layers.8.self_attn.v_proj', 'model.decoder.layers.9.self_attn.v_proj', 'model.decoder.layers.10.self_attn.v_proj', 'model.decoder.layers.11.self_attn.v_proj']\n",
            "['model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.4.self_attn.q_proj', 'model.decoder.layers.5.self_attn.q_proj', 'model.decoder.layers.6.self_attn.q_proj', 'model.decoder.layers.7.self_attn.q_proj', 'model.decoder.layers.8.self_attn.q_proj', 'model.decoder.layers.9.self_attn.q_proj', 'model.decoder.layers.10.self_attn.q_proj', 'model.decoder.layers.11.self_attn.q_proj', 'model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.4.self_attn.k_proj', 'model.decoder.layers.5.self_attn.k_proj', 'model.decoder.layers.6.self_attn.k_proj', 'model.decoder.layers.7.self_attn.k_proj', 'model.decoder.layers.8.self_attn.k_proj', 'model.decoder.layers.9.self_attn.k_proj', 'model.decoder.layers.10.self_attn.k_proj', 'model.decoder.layers.11.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.4.self_attn.v_proj', 'model.decoder.layers.5.self_attn.v_proj', 'model.decoder.layers.6.self_attn.v_proj', 'model.decoder.layers.7.self_attn.v_proj', 'model.decoder.layers.8.self_attn.v_proj', 'model.decoder.layers.9.self_attn.v_proj', 'model.decoder.layers.10.self_attn.v_proj', 'model.decoder.layers.11.self_attn.v_proj']\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "lora_A\n",
            "lora_B\n",
            "Total Parameters: 125681664\n",
            "Trainable Parameters: 442368\n"
          ]
        }
      ],
      "source": [
        "#inject_adapter(causal_model, [\"q_proj_k_proj_v_proj\"], lambda x: LoRAAdapter(x, r=8, lora_alpha=8, in_features=x.in_features, out_features=x.out_features))\n",
        "inject_adapter(causal_model, ['q_proj','k_proj','v_proj'], lambda x: LoRAAdapter(x, r=8, lora_alpha=8, in_features=x.in_features, out_features=x.out_features))\n",
        "mark_only_lora_as_trainable(causal_model)\n",
        "\n",
        "# TODO: Calculate total parameters and total trainable parameters\n",
        "total_params = sum(p.numel() for p in causal_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in causal_model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kyNHGoJxS1h"
      },
      "source": [
        "Finally, run the cell below to check the new model's architecture. If the key, value, and query matrices are all now replaced by a LoRA adapter, you are good to go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mlWkgBe3c_jX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0340e2e6-1138-483e-b160-07157fa6d482"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
              "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): LoRAAdapter(\n",
              "              (existing_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (v_proj): LoRAAdapter(\n",
              "              (existing_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (q_proj): LoRAAdapter(\n",
              "              (existing_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Check the new model architecture\n",
        "causal_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94dHW2X26T9G"
      },
      "source": [
        "## 5.4: Finetuning your LoRA adapters on Wikitext\n",
        "\n",
        "In this next part, we will finally finetune the LoRA adapter of our causal model on a small subset of the training set of Wikitext. If all went correctly, we should notice that the perplexity over our test set went down!\n",
        "\n",
        "Since we are only using a small subset of the training set and a low chunk size, you shouldn't expect the perplexity to go down by much (<1 point).\n",
        "\n",
        "**Note:** Please be aware that this code may take some time to run (you are literally training a large language model), so please be fully confident in your completed code above.  With this being said **please ensure that you record the final perplexity score** (you may even want to screenshot it for proof).\n",
        "\n",
        "First, let's define our finetuning function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "70Bt9eOm6OO6"
      },
      "outputs": [],
      "source": [
        "def finetune_causal_model(model, train_dataset, epochs=1, learning_rate=1e-4):\n",
        "        def tokenize_function(examples):\n",
        "            result = causal_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256) #256 chosen for Colab's GPU size\n",
        "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "            return result\n",
        "\n",
        "        train_dataset = Dataset.from_dict(train_dataset)\n",
        "        tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "        data_collator = DataCollatorForLanguageModeling(causal_tokenizer, mlm=False)\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"/content\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            per_device_train_batch_size=8,\n",
        "            learning_rate=learning_rate,\n",
        "            weight_decay=0.01,\n",
        "            num_train_epochs=epochs,\n",
        "        )\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            eval_dataset=tokenized_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "        trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D0i9FoQ6cIZ"
      },
      "source": [
        "Next, let's load our training dataset.\n",
        "\n",
        "A few interesting things to note: The training dataset can be quite large with respect to our compute resources, so we're only going to use a small fraction of it.  Also, we are going to split our text into chunks so that the attention gradients can fit on Colab's GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DMLmyU3Z6czx"
      },
      "outputs": [],
      "source": [
        "wiki_training_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "chunks = []\n",
        "\n",
        "# As big as Colab's GPU can fit\n",
        "chunk_size = 256\n",
        "\n",
        "\n",
        "def split_into_chunks(text, chunk_size):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "for example in wiki_training_dataset:\n",
        "    text = example['text']\n",
        "    text_chunks = split_into_chunks(text, chunk_size)\n",
        "    chunks.extend(text_chunks)\n",
        "\n",
        "processed_train_dataset = {'text':chunks[:len(chunks)//10]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIZgPDKE6ejl"
      },
      "source": [
        "Finally, calculate the score of our new model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import wikitext dataset\n",
        "causal_test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "causal_test_encodings = causal_tokenizer(\"\\n\\n\".join(causal_test[\"text\"]), return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "S3uWLCXetp0w"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_perplexity(model, encodings, stride):\n",
        "    max_length = 1024\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(\"cuda\")\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            neg_log_likelihood = outputs.loss\n",
        "\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "    return torch.exp(torch.stack(nlls).mean())"
      ],
      "metadata": {
        "id": "ZuxgdohUtZjW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dpO2jq4T6fs3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "f59fcc9952384bf3846142962681cba1",
            "a756b04b75264d38ae18f9b1a78379c1",
            "e784ab476fc54b59868867d6e6bbcb4c",
            "271ff15703944f8297ffca80cdd1fe2f",
            "cf63a3cb747147f3bb0397b6824f6655",
            "7a2cd5e35f6f49ac9e099a978579b75b",
            "02abc57b25fd4c50a96b7a7ba9e9ef5c",
            "c68427ae82354b1793f905d73bf4d5dc",
            "73314b42319548a2b073b9ef961cf48c",
            "e42db56331324aa6b4fcd02fb5d38b11",
            "3e40789b177349449ce6fbdd23624d7f"
          ]
        },
        "outputId": "e549982c-fd3c-4c2c-e4fe-3264255b4266"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f59fcc9952384bf3846142962681cba1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5752 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='719' max='719' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [719/719 08:37, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.353200</td>\n",
              "      <td>4.172896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 1120/1124 [02:33<00:00,  7.32it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(24.2500, device='cuda:0', dtype=torch.bfloat16)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "finetune_causal_model(causal_model, processed_train_dataset)\n",
        "calc_perplexity(causal_model, causal_test_encodings, 256)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LV6kG01kD0Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJOKbcmfJ8Px"
      },
      "source": [
        "## 5.5 Conceptual Questions\n",
        "**Question:** What do you think the benefits of using LoRA are?  What might be some drawbacks?\n",
        "\n",
        "  **Your Answer:**\n",
        "\n",
        "  No additional inference time, since we do not add additional structure; less fine tune computation cost; and almost same effect of the full fine tune.\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Question:** Discuss the trade-offs between model size, speed, and accuracy when using LoRA in LLMs.\n",
        "\n",
        "  **Your Answer:**\n",
        "\n",
        "  Advantages:\n",
        "\n",
        "Drastically reduced parameter count (often less tha. 1 percent of original model)\n",
        "Much smaller storage requirements for task-specific adaptations\n",
        "Multiple tasks can share the same base model\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Still needs to load full base model in memory\n",
        "Each task requires separate LoRA weights\n",
        "May need larger rank for complex tasks\n",
        "\n",
        "\n",
        "\n",
        "Speed Trade-offs:\n",
        "\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Faster training (up to 25% faster than full fine-tuning)\n",
        "No additional inference latency when merged Quick task switching by loading different LoRA weights\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Initial model loading still takes same time\n",
        "Memory I/O can be a bottleneck when switching tasks\n",
        "Merging weights adds minor overhead\n",
        "\n",
        "\n",
        "Accuracy Trade-offs:\n",
        "\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Can match or exceed full fine-tuning performance\n",
        "More stable training due to fewer parameters\n",
        "Less prone to catastrophic forgetting\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "May require tuning rank size for optimal performance\n",
        "Not all layers benefit equally from LoRA\n",
        "Some complex tasks might need full fine-tuning\n",
        "\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "29kXZvBvi-ey"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f59fcc9952384bf3846142962681cba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a756b04b75264d38ae18f9b1a78379c1",
              "IPY_MODEL_e784ab476fc54b59868867d6e6bbcb4c",
              "IPY_MODEL_271ff15703944f8297ffca80cdd1fe2f"
            ],
            "layout": "IPY_MODEL_cf63a3cb747147f3bb0397b6824f6655"
          }
        },
        "a756b04b75264d38ae18f9b1a78379c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a2cd5e35f6f49ac9e099a978579b75b",
            "placeholder": "​",
            "style": "IPY_MODEL_02abc57b25fd4c50a96b7a7ba9e9ef5c",
            "value": "Map: 100%"
          }
        },
        "e784ab476fc54b59868867d6e6bbcb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c68427ae82354b1793f905d73bf4d5dc",
            "max": 5752,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73314b42319548a2b073b9ef961cf48c",
            "value": 5752
          }
        },
        "271ff15703944f8297ffca80cdd1fe2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e42db56331324aa6b4fcd02fb5d38b11",
            "placeholder": "​",
            "style": "IPY_MODEL_3e40789b177349449ce6fbdd23624d7f",
            "value": " 5752/5752 [00:01&lt;00:00, 3804.55 examples/s]"
          }
        },
        "cf63a3cb747147f3bb0397b6824f6655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2cd5e35f6f49ac9e099a978579b75b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02abc57b25fd4c50a96b7a7ba9e9ef5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c68427ae82354b1793f905d73bf4d5dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73314b42319548a2b073b9ef961cf48c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e42db56331324aa6b4fcd02fb5d38b11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e40789b177349449ce6fbdd23624d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}