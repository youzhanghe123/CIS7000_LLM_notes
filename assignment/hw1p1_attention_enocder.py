# -*- coding: utf-8 -*-
"""hw1p1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aSSaKuu8wVNuBDiWnCbm6oKxmHf6OjCo

<h1 align="center" style="color:green;font-size: 3em;">Homework 1:
Implementing Transformers From Scratch Using PyTorch</h1>


# Part 1: Introduction

In this homework, you will implement the transformer architecture as described in the "Attention is All You Need" paper from scratch using PyTorch.

**Instructions:**
- Follow the notebook sections to implement various components of the transformer.
- Code cells marked with `TODO` are parts that you need to complete.
- Ensure your code runs correctly by the end of the notebook.
- Download both the .ipynb and .py versions of the notebook and submit on Gradescope


![](https://drive.google.com/uc?export=view&id=1RXzpkfpzo0cvoUUHbs42AKCC_D6ghs4P)

# Part 2: Import Libraries
"""

# importing required libraries
import torch.nn as nn
import torch
import torch.nn.functional as F
import math,copy,re
import warnings
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import random
warnings.simplefilter("ignore")
print(torch.__version__)

# Set the seed value
seed_value = 0

# For CPU
torch.manual_seed(seed_value)
random.seed(seed_value)
np.random.seed(seed_value)

# For GPU (if using CUDA)
if torch.cuda.is_available():
    torch.cuda.manual_seed(seed_value)
    torch.cuda.manual_seed_all(seed_value)
    torch.backends.cudnn.deterministic = True

"""We know that transformers have an encoder-decoder architecture for language translation. Before diving into the encoder or decoder, let us discuss some common components to process the input to the transformer.


# Part 3: Basic Transformer Class

Neural networks operate over numerical weights and biases but natural language does not naturally take this form. Thus first, we need to convert an input sequence into an embedding vector. Embedding vectors create a more semantic representation of each token.

## 3.1: Embeddings Matrix


**3.1: Understanding Embeddings**

Transformers require numerical input, but natural language consists of words. To convert words into a numerical format, we use embeddings, which map each word to a high-dimensional vector. These vectors capture semantic information about the words.

**Task:**

- Instantiate an embedding layer using the `nn.Embedding` class in PyTorch.
- Investigate the properties of the embedding matrix.
- Embed multiple tokens and analyze the results.

**Step-by-Step Instructions:**

1. **Create an Embedding Layer:**
    - Assume a vocabulary size of 100 and an embedding dimension of 512.
    - Create an embedding layer using `nn.Embedding`.

2. **Analyze the Embedding Matrix:**
    - Print the shape of the embedding matrix.
    - Extract and print the first 3 rows of the embedding matrix (corresponding to tokens 0, 1, and 2).
  
3. **Embed Multiple Tokens:**
    - Create an input tensor representing a sequence of tokens. For example, use the tokens `[0, 1, 2]`.
    - Pass this input through the embedding layer.
    - Print and compare the embedding vectors for tokens 0, 1, and 2 with the first 3 rows of the embedding matrix.
"""

# TODO: Create an Embedding Layer
vocab_size = 100
embed_dim = 512

# TODO: Print shape of the Embedding Matrix
embedding_layer = nn.Embedding(vocab_size, embed_dim)
print("Embedding Matrix Shape:", )

# TODO: Print first 3 rows of embedding matrix
print("First 3 Rows of Embedding Matrix:")
print(embedding_layer.weight[:3,:])

# TODO: Embed Multiple Tokens
input_tensor = torch.tensor([0, 1, 2])
embedded_tokens = embedding_layer(input_tensor)
print("\nEmbedding Vectors for Tokens 0, 1, 2:")
print(embedded_tokens)

## Run the small test case below to make sure everything is working correctly
assert embedding_layer.weight.shape == (vocab_size, embed_dim), "Embedding layer's shape does not match"
assert embedded_tokens.shape == (3, embed_dim), "Embedded token's shape does not match"

"""**Discussion Questions:**
- What do you observe about the embedding vectors for tokens 0, 1, and 2?
- How do these vectors compare to the corresponding rows in the embedding matrix? Why is this?

The vector of the first token is identical to the first row of the embedding matrix, the vector of the second token is identical to the second row. This is because the embedding layer essentially works as a lookup table. It maps each token to a pre-initialized vector stored in the embedding matrix. When we pass in a token index, the layer retrieves the corresponding row from the embedding matrixã€‚

## 3.2: Positional Encoding

The next step is to generate positional encoding. For the model to understand a sentence, it helps to know two things about each token:
- What does the token mean semantically?
- What is the position of the token in the sentence?

In the "Attention is All You Need" paper, the authors used the following functions to create positional encoding. A cosine function is used for odd time steps, and a sine function is used for even time steps.

<img src="https://miro.medium.com/max/524/1*yWGV9ck-0ltfV2wscUeo7Q.png">

<img src="https://miro.medium.com/max/564/1*SgNlyFaHH8ljBbpCupDhSQ.png">

```
pos -> refers to order in the sentence
i -> refers to position along embedding vector dimension
```

Positional encoding will generate a matrix similar to the embedding matrix. It will create a matrix of dimension sequence length x embedding dimension. For each token (word) in the sequence, we will find the embedding vector, which is of dimension (1, 512), and add it with the corresponding positional vector, which is also of dimension (1, 512), to get a (1, 512) dimension output for each word/token.

For example, if we have a batch size of 32 and a sequence length of 10 with an embedding dimension of 512, we will have an embedding vector of dimension (32, 10, 512). Similarly, we will have a positional encoding vector of dimension (32, 10, 512). Then we add both.

<img src="https://miro.medium.com/max/906/1*B-VR6R5vJl3Y7jbMNf5Fpw.png">

<hr>
<h3>Task:</h3>
Implement the `PositionalEmbedding` class. Complete the `__init__` method to initialize the positional encoding matrix and the `forward` method to add positional encoding to the input embeddings.

```
Code Hint:
Use math.sin and math.cos functions to create the positional encoding matrix.
```

```
Code Hint:
Ensure the positional encoding matrix is creating without gradients. Look at pytorch buffers for more information.
```


**Note:** Ensure that the positional encoding matrix is not trained by the optimizer by registering it as a buffer.
"""

import torch
import torch.nn as nn
import math

class PositionalEmbedding(nn.Module):
    def __init__(self, max_seq_len, embed_model_dim):
        """
        Args:
            max_seq_len: maximum length of input sequence
            embed_model_dim: dimension of embeddings
        """
        super(PositionalEmbedding, self).__init__()
        self.embed_dim = embed_model_dim

        # TODO: Initialize the positional encoding matrix using the above equation
        pe = torch.zeros(max_seq_len, embed_model_dim)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1) #here is adding the embedding dimension
        div_term = torch.exp(torch.arange(0, embed_model_dim, 2).float() * (-math.log(10000.0) / embed_model_dim)) #this is the computation of 1000**(-2i/d)

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # Add a batch dimension to the positional encoding
        pe = pe.unsqueeze(0)

        # TODO: Register the positional encoding matrix as a buffer
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: input vector
        Returns:
            x: output vector with positional encoding added
        """

        # Weight the embeddings relatively larger
        x = x * math.sqrt(self.embed_dim)

        # TODO: Add positional encoding to the input embeddings
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len]
        return x

## Run the small test case below to make sure everything is working correctly
max_seq_len = 10
embed_model_dim = 16
batch_size = 2

# Initialize the PositionalEmbedding class
pos_emb = PositionalEmbedding(max_seq_len, embed_model_dim)

# Create a dummy input tensor
x = torch.randn(batch_size, max_seq_len, embed_model_dim)
output = pos_emb(x)

assert output.shape == x.shape, "Output shape does not match input shape"
assert not torch.equal(output, x), "Positional encoding was not added correctly"

"""## 3.3 What is Self-Attention?
In this section, we will explore Self-Attention and Multi-Head Attention mechanisms.

Consider the sentence, "The cat slept because it was tired." Here, "it" refers to "the cat." While this is intuitive for humans, this may not be clear at all to a machine.

Self-Attention allows the model to consider other positions in the input sequence while processing each word, enabling it to generate a vector that captures dependencies between words.

*Include comparison to prior nlp models

Let's break down the self-attention mechanism step by step:

1. **Input Projections:** For each word in the input, create three vectors: Query (Q), Key (K), and Value (V). Each vector has a dimension of 1x64.

   In multi-head attention, we have multiple self-attention heads (e.g., 8 heads). Each head creates separate Q, K, and V vectors.

   **How to create Key, Query, and Value vectors?**

   Use matrices (key, query, and value matrices) to generate these vectors. These matrices are learned during training.
```
Code Hint:
If batch_size=32, sequence_length=10, and embedding_dimension=512, the output after embedding and positional encoding will be 32x10x512.
Resize it to 32x10x8x64 (8 is the number of heads in multi-head attention).
```


* **Step 2:** **Calculate Attention Scores:** Multiply the query matrix with the key matrix transpose: [Q x K.t]
```
Code Hint:
If the dimensions of key, query, and value are 32x10x8x64, transpose them to 32x8x10x64.
Then, multiply the query matrix with the key matrix transpose: (32x8x10x64) x (32x8x64x10) -> (32x8x10x10).
```


* **Step 3:**  **Scale Scores:** Divide the output matrix by the square root of the key matrix dimension and apply Softmax.
```
Code Hint:
Divide the 32x8x10x10 vector by 8 (the square root of 64, the key matrix dimension).
```


* **Step 4:** **Weighted Sum:** Multiply the scores with the value matrix.
```
Code Hint:
After step 3, the output will be 32x8x10x10. Multiply it with the value matrix (32x8x10x64) to get the output (32x8x10x64).
```

* **Step 5:** **Output Transformation:** Pass the result through a linear layer to form the final output of the multi-head attention.
```
Code Hint:
Transpose the (32x8x10x64) vector to (32x10x8x64) and reshape it to (32x10x512).
Then, pass it through a linear layer to get the output of (32x10x512).
```


Now that you have an overview of how multi-head attention works, let's implement it. You will gain a deeper understanding through the following code exercise.

<hr>
<h3>Task:</h3>
Implement the `MultiHeadAttention` class. Complete the `__init__` and `forward` methods to perform the multi-head self-attention operation.

```
Code Hint:
Ensure you properly reshape and transpose the tensors to match the required dimensions for matrix multiplication.
```

**Note:** Masking can be used in the decoder to prevent attending to future tokens, but more on this later

"""

class MultiHeadAttention(nn.Module):
 def __init__(self, embed_dim=512, n_heads=8):
     """
     Args:
         embed_dim: dimension of embedding vector output
         n_heads: number of self-attention heads
     """
     super(MultiHeadAttention, self).__init__()

     self.embed_dim = embed_dim    # 512 dim
     self.n_heads = n_heads   # 8 heads
     self.single_head_dim = embed_dim // n_heads   # 512 / 8 = 64, each key, query, and value will be 64d

     # TODO: Initialize key, query, and value matrices
     self.query_matrix = nn.Linear(self.embed_dim, self.embed_dim)
     self.key_matrix = nn.Linear(self.embed_dim, self.embed_dim)
     self.value_matrix = nn.Linear(self.embed_dim, self.embed_dim)
     self.out = nn.Linear(self.embed_dim, self.embed_dim)

 def forward(self, key, query, value, mask=None):
     """
     Args:
        key: key vector
        query: query vector
        value: value vector
        mask: mask for decoder

     Returns:
        output: vector from multi-head attention
     """
     batch_size = key.size(0)
     seq_length = key.size(1)
     seq_length_query = query.size(1)

     # TODO: Apply linear transformations
     k = self.key_matrix(key)
     q = self.query_matrix(query)
     v = self.value_matrix(value)

     # TODO: Reshape key, query, and value
     k = k.reshape(batch_size, seq_length, self.n_heads, self.single_head_dim )
     q = q.reshape(batch_size, seq_length, self.n_heads, self.single_head_dim )
     v = v.reshape(batch_size, seq_length, self.n_heads, self.single_head_dim )

     q = q.transpose(1, 2) #now k is of the shape (batch_size, self.n_heads, seq_length, self.single_head_dim )
     k = k.transpose(1, 2)
     v = v.transpose(1, 2)
     # TODO: Compute attention scores
     k_adjusted = k.transpose(-1, -2) # k adjusted is of the shape (batch_size, self.n_heads, self.single_head_dim, seq_length )
     product = torch.matmul(q, k_adjusted) # porduct is of the shape (batch_size, self.n_heads, seq_length, seq_length )

     #More about attention masks later
     if mask is not None:
         product = product.masked_fill(mask == 0, float("-1e20"))


     product = product/math.sqrt(self.single_head_dim)
     scores = F.softmax(product, dim=-1)

     # TODO: Compute weighted sum of value vectors and run it through the last layer
     scores = torch.matmul(scores, v) # scores is of the shape (batch_size, self.n_heads, seq_length, single_head_dim )


     concat = scores.transpose(1,2).reshape(batch_size, seq_length, -1 )
     output = self.out(concat)

     return output

## Run the small test case below to make sure everything is working correctly
embed_dim = 512
n_heads = 8
batch_size = 2
seq_length = 10

# Initialize the MultiHeadAttention module
mha = MultiHeadAttention(embed_dim=embed_dim, n_heads=n_heads)
mha.eval()

# Create dummy tensors
key = torch.randn(batch_size, seq_length, embed_dim)
query = torch.randn(batch_size, seq_length, embed_dim)
value = torch.randn(batch_size, seq_length, embed_dim)

with torch.no_grad():
  output = mha(key, query, value)
assert output.shape == (batch_size, seq_length, embed_dim), "Output shape does not match the expected shape"

"""That's all for HW1 Part 1! Please submit both the .py and .ipynb on Gradescope and note any score there may not be your final score on the assignment. Final scores will be released when the overall homework is due."""